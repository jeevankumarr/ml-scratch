{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is bias variance trade-off?\n",
    "\n",
    "* The Bias-Variance trade-off is the central problem in supervised machine learning. Ideally, one wants to choose a model that accurately captures the regularities in its training data, but also generilizes well to unseen data. Unfortunately, these goals are contradictory, and often impossible to do both. \n",
    "* __Bias__ represents the error as a result of misaligned assumptions in the learning algorithm that do not represent the true relationship between predictors and the response variable. \n",
    "* __Variance__ represents the error from sensitivity to fluctuations in the training set. High variance can cause an algorithm to model the noise in the training data, rather than intended outputs. \n",
    "* __Discussion__ Models with low-bias are usually more complex (ex: higher-order regression polynomials), enabling them to represent the training set from accurately. However, they may also represent the noise in the trianing set, making their predictions less accurate. On the other hand, models with high-bias are simple (ex: linear regression polynomials), but may produce lower variance prediction when applied beyond the training set. \n",
    "* __Sources:__ [wiki](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Derive bias variance trade-off?\n",
    "\n",
    "\n",
    "* __Derivation of Variance:__  \n",
    "$Var(X) = E[(X-E[X])^2]$  \n",
    "$Var(X) = E[(X^2 + E[X]^2 - 2 * X E[X])]$  \n",
    "$Var(X) = E[X^2] + E[X]^2 - 2 * E[X] * E[X]$  \n",
    "$Var(X) = E[X^2] - E[X]^2$  \n",
    "\n",
    "* __Variance in $y$__  \n",
    "$ Var(y) = E[ (y - E[y])^2]$  \n",
    "$ Var(y) = E[ (E[y] + e - E[y])^2]$  \n",
    "$ Var(y) = E[e^2]$  \n",
    "$ Var(y) = Var(e) + E[e]^2$; But: $E[e] = 0$  \n",
    "$ Var(y) = Var(e)$  \n",
    "\n",
    "* __Sum of Squares Error:__  \n",
    "$SS = E[(y - \\hat{y})^2]$  \n",
    "$SS = E[(y^2 + \\hat{y}^2 - 2 * y * \\hat{y}]$  \n",
    "$SS = E[y^2] + E[\\hat{y}^2] - 2 E(y*\\hat{y})$  \n",
    "$SS = Var(y) + E[y]^2  + Var(\\hat{y}) + E[\\hat{y}]^2  - 2 E(y*\\hat{y})$\n",
    "$SS = Var(e) + Var(\\hat{y}) + \\left( E[y] - E[\\hat{y}]\\right)^2$  \n",
    "$SS = irreducible\\space error + Variance + Bias^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Linear regression algorithm with stochastic gradient decent\n",
    "$J(\\theta) = \\sum_{i=1}^{m} L(\\hat{y_i} - y_i)$  \n",
    "$\\space \\space \\theta = \\theta - \\alpha * \\left(\\hat{y_i} - y_i  \\right) ^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficients_sgd(train, learning_rate, iter, pred):\n",
    "    \"\"\"Stochastic Gradient Descent\n",
    "    Args:\n",
    "        train (numpy.ndarray): input data\n",
    "        learning_rate (float): rate of learning\n",
    "        iter (int): No. of iterations\n",
    "    Returns:\n",
    "        list, float:\n",
    "    \"\"\"\n",
    "    coef = [0.0] * len(train[0])\n",
    "    sum_err = 0.0\n",
    "    for i in range(iter):\n",
    "        sum_err = 0.0\n",
    "        for row in train:\n",
    "            yhat = pred(row, coef)\n",
    "            err = yhat - row[-1]\n",
    "            sum_err += err ** 2\n",
    "            coef[0] = coef[0] - learning_rate * err\n",
    "\n",
    "            for j in range(len(row) - 1):\n",
    "                coef[j + 1] = coef[j + 1] - learning_rate * err * row[j]\n",
    "    return coef, sum_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Simple linear regression derivation \n",
    "\n",
    "$y = b_0 + b_1 x$  \n",
    "$RSS = \\sum_{i=1}^{n} \\left( y_i - \\hat{y_i} \\right)$  \n",
    "$\\frac{\\partial{RSS}}{\\partial{b_0}} = \\sum_{i=1}^{n} -2 \\left[ y_i - b_0 - b_1x_i \\right] = 0$  \n",
    "$\\frac{\\partial{RSS}}{\\partial{b_0}} = 2 \\left[ nb_0 + b_1 \\sum_{i=1}^{n}  x_i - \\sum_{i=1}^{n} y_i \\right] = 0$  \n",
    "$b_0 = \\frac{\\sum_{i=1}^{n} y_i}{n} - b_1 \\frac{\\sum_{i=1}^{n} x_i}{n}$  \n",
    "$b_0 = \\bar{y} - b_1 \\bar{x}$ \n",
    "  \n",
    "$\\frac{\\partial{RSS}}{\\partial{b_1}} = \\sum_{i=1}^{n} -2 x_i \\left[ y_i - b_0 - b_1x_i \\right] = 0$  \n",
    "$\\frac{\\partial{RSS}}{\\partial{b_1}} = \\sum_{i=1}^{n} -2 x_i \\left[ y_i - b_0 - b_1x_i \\right] = 0$  \n",
    "$\\frac{\\partial{RSS}}{\\partial{b_1}} = \\sum_{i=1}^{n} -2 x_i \\left[ y_i - \\bar{y} + b_1 \\bar{x} - b_1x_i \\right] = 0$  \n",
    "  \n",
    "$\\frac{\\partial{RSS}}{\\partial{b_1}} = -2 \\sum_{i=1}^{n} \\left[ x_iy_i - \\bar{y} x_i + b_1\\bar{x}x_i - b_1x_i^2 \\right] = 0$  \n",
    "$\\sum_{i=1}^{n} \\left[ x_iy_i - \\bar{y} x_i \\right] - b_1 \\sum_{i=1}^{n} \\left[ b_1x_i^2 - \\bar{x}x_i \\right] = 0$  \n",
    "$b_1 = \\frac{\\sum_{i=1}^{n} \\left( x_iy_i - \\bar{y} x_i \\right)}{\\sum_{i=1}^{n} \\left( x_i^2 - \\bar{x}x_i \\right)}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Gini Impurity\n",
    "\n",
    "$gini\\_impurity = 1 - \\sum_{i}^{C} p_i^2$\n",
    "\n",
    "$gini\\_index:$\n",
    "```\n",
    "for each branch in split:  \n",
    "    Calculate percent branch represents (Used for weighting)\n",
    "    Calculate gini_impurity\n",
    "Weight each branch's gini_impurity by the share of samples it represents\n",
    "Sum the weighted gini index for each split.\n",
    "```\n",
    "\n",
    "$cross\\_entropy  = \\sum_{i}^{C} - p_i \\log(p_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. CART\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decision_tree(train, max_depth, min_leaf_size): \n",
    "    root = get_best_split(train)\n",
    "    split(root, max_depth, min_leaf_size, 1)\n",
    "    return root\n",
    "\n",
    "def split(node, max_depth, min_leaf_size, level):\n",
    "    left, right = node.left, node.right\n",
    "    \n",
    "    if left is None or right is None: to_terminal(node)\n",
    "    \n",
    "    if level > max_depth: return to_terminal(node)\n",
    "    \n",
    "    #Do left and right\n",
    "    if len(left) <= min_leafe_size: node.left = to_terminal(node)\n",
    "    else: \n",
    "        node.left = get_split(left)\n",
    "        split(node.left, max_depth, min_leaf_size, level + 1)\n",
    "\n",
    "def get_split(data):\n",
    "    classes = data['class'].unique()\n",
    "    gini_lowest = 'inf'\n",
    "    split_groups = (None, None)\n",
    "    split_index, split_val = None, None\n",
    "    \n",
    "    for col in data.columns:\n",
    "        for row in data:\n",
    "            a, b = make_split(data, col, row[col])\n",
    "            gini = calc_gini_index([a, b], classes)\n",
    "            if gini < gini_lowest:\n",
    "                gini_lowest = gini\n",
    "                split_groups = (a, b)\n",
    "                split_index = col\n",
    "                split_val = row[col]\n",
    "    return {'groups':split_groups, 'index': split_index, 'val': split_val}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Bagged Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(train, test, max_depth, min_size, sample_size, n_trees):\n",
    "    trees = []\n",
    "    for _ in range(n_trees):\n",
    "        sample = subsample(train, sample_size)\n",
    "        tree = build_tree(sample, max_depth, min_size)\n",
    "        trees.append(tree)\n",
    "    predictions = [bagging_predict(trees, row) for row in test.values]\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(train, test, max_depth, min_size, sample_ratio, tree_ct,\n",
    "                  feature_ct):\n",
    "    \"\"\"Build a random forest from the training dataset and predict outcomes for\n",
    "    the test dataset\n",
    "\n",
    "    Args:\n",
    "        train (pandas.DataFrame): training dataset\n",
    "        test (pandas.DataFrame): test dataset\n",
    "        max_depth (int): max depth of any tree\n",
    "        min_size (int): min size of the dataset at the terminal node\n",
    "        sample_ratio (float): ratio of re-sampling\n",
    "        tree_ct (int): no. of trees in the foredst\n",
    "        feature_ct (int): no. of features for each tree\n",
    "\n",
    "    Returns:\n",
    "        (list): list of predictions\n",
    "    \"\"\"\n",
    "    trees = []\n",
    "\n",
    "    for _ in range(tree_ct):\n",
    "        sample = subsample(train, sample_ratio)\n",
    "        tree = build_forest(sample, max_depth, min_size, feature_ct)\n",
    "        # print_tree(tree)\n",
    "        trees.append(tree)\n",
    "\n",
    "    predictions = [bagging_predict(trees, row) for row in test.values]\n",
    "    return predictions\n",
    "def build_forest(train, max_depth, min_size, n_features):\n",
    "    \"\"\"Build a tree\n",
    "\n",
    "    Args:\n",
    "        train (pandas.DataFrame): training dataset\n",
    "        max_depth (int): max depth of the tree\n",
    "        min_size (int): min size of samples in terminal node\n",
    "        n_features (int): no. of features for each tree\n",
    "\n",
    "    Returns:\n",
    "        dict: tree with left and right sub-trees\n",
    "    \"\"\"\n",
    "    root = get_split(train.values, n_features)\n",
    "    _build_forest(root, max_depth, min_size, n_features, 1)\n",
    "    return root\n",
    "\n",
    "def get_split(dataset, n_features):\n",
    "    \"\"\"This method randomly picks n_features and returns the best split across\n",
    "    all the features\n",
    "\n",
    "    Args:\n",
    "        dataset(numpy.ndarray): input dataset\n",
    "        n_features (int): no. of features to be considered for splitting\n",
    "\n",
    "    Returns:\n",
    "        dict: of split parameters\n",
    "    \"\"\"\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_idx, b_val, b_score, b_groups = 999, 999, 999, None\n",
    "\n",
    "    indicies = random.sample(range(len(dataset[0]) - 1), n_features)\n",
    "    features = indicies\n",
    "    # print(\"features:\", features)\n",
    "    for feature in features:\n",
    "        for row in dataset:\n",
    "            groups = split_data(dataset, feature, row[feature])\n",
    "            gini = gini_index(groups, class_values)\n",
    "\n",
    "            if gini < b_score:\n",
    "                b_idx, b_val, b_score, b_groups = feature, row[feature], gini, groups\n",
    "\n",
    "    return {\"index\": b_idx, \"value\": b_val, \"groups\": b_groups}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\log\\left( \\frac{p}{1-p}\\right) = \\beta^T X$  \n",
    "\n",
    "Model:   \n",
    "$y_i \\sim Binomial(m_i, p_i) \\space \\forall \\space i = 0, 1, 2, ...m; m = no. of samples$  \n",
    "  \n",
    "$Pr[Y_1 = y_1, Y_2 = y_2... Y_m = y_m | p_1, p_2, p_3...p_m] = \\prod_{i=1}^{m} p_i^{y_i} (1-p_i)^{1-y_i} = \\mathcal{L} $  \n",
    "  \n",
    "$\\log(\\mathcal{L} ) = \\sum_{i=1}^{m} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]$  \n",
    "  \n",
    "$\\log(\\mathcal{L} ) = \\sum_{i=1}^{m} \\left[ y_i \\log \\left( \\frac{p_i}{1-p_i}\\right) + \\log(1-p_i) \\right]$  \n",
    "  \n",
    "$\\log(\\mathcal{L} ) = \\sum_{i=1}^{m} \\left[ y_i \\beta^T x_i + \\log\\left(1-\\frac{\\exp{\\beta^T X_i}}{1+ \\exp{\\beta^T X_i}}\\right) \\right]$  \n",
    "  \n",
    "$\\log(\\mathcal{L} ) = \\sum_{i=1}^{m} \\left[ y_i \\beta^T x_i - \\log\\left(1+\\exp{\\beta^T X_i}\\right) \\right]$  \n",
    "  \n",
    "$\\frac{\\partial{\\log(\\mathcal{L})}}{\\partial{\\beta}} = 0 = \\sum_{i=1}^{m} \\left[y_i x_i - \\frac{\\exp(\\beta^Tx_i)}{1+\\exp(\\beta^Tx_i)}  \\right]$  \n",
    "  \n",
    "$\\frac{\\partial{\\log(\\mathcal{L})}}{\\partial{\\beta}} = 0 = \\sum_{i=1}^{m} \\left[x_i \\left(y_i - p(x_i; \\beta) \\right) \\right]$  \n",
    "  \n",
    "$\\frac{\\partial^2{\\log(\\mathcal{L})}}{\\partial{\\beta} \\partial{\\beta}} = - \\sum_{i=1}^{m} x_i\\frac{-\\exp(\\beta^Tx_i)}{(1+ \\exp(\\beta^Tx_i)) (1+ \\exp(\\beta^Tx_i))}$  \n",
    "$\\frac{\\partial^2{\\log(\\mathcal{L})}}{\\partial{\\beta} \\partial{\\beta}} = - \\sum_{i=1}^{m} x_i p_i (1-p_i)$  \n",
    "  \n",
    "$\\beta_{iter + 1} = \\beta_{iter} - \\frac{\\partial{\\log(\\mathcal{L})}}{\\partial^2{\\log(\\mathcal{L})}}$  : Newton method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Derive NN - backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. SVN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Impact of a given feature on response variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Types of error calcualtion\n",
    "\n",
    "* __MSE (Mean Squared Error)__: $\\frac{\\sum_{i=1}^{m} (y_i - \\hat{y_i})^2}{m}$ \n",
    "    * Includes both variance of the estimator (how widely spread the estimates are from one data sample to another) and bias of the estimator (how far off the average estimated value is from the truth) \n",
    "    * RMSE also represents the standard error of the estimator\n",
    "    * Inflates large errors (or outliers)\n",
    "    * Squaring is nicer than taking the absolute value, e.g. it is smooth. It also leads to a definition of variance which has nice mathematical properties, e.g. it is additive. But for me the theorem that really justifies using standard deviation over the mean absolute error is the central limit theorem. The central limit theorem is at work whenever we measure the mean and standard deviation of a distribution we assume to be normal (e.g. heights in a population) and use that to make predictions about the entire distribution, since a normal distribution is completely specified by its mean and standard deviation.\n",
    "* __MAD__: Mean Absolute Deviation $\\frac{\\sum_{i=1}^{m} \\mid y_i - \\hat{y_i} \\mid}{m}$ \n",
    "    * Resistent to outliers\n",
    "* __MAPE__: Mean Absolute Percentage error $\\frac{\\sum_{i=1}^{m} \\mid \\frac{(y_i - \\hat{y_i}) }{y_i} \\mid}{m}$ \n",
    "    * Works well when the response variable is symetric and normally distributed\n",
    "    * Does not work for skewed distribution\n",
    "    * If the response variable has 0 then MAPE has no value\n",
    "    * Direct business interpretation (for ex: % of error in the sales forecast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. How to evaluate a binary classifier?\n",
    "\n",
    "* __Basic Metrics__\n",
    "\n",
    "|Metric|Formula|Intepretation|\n",
    "|-------|-----|----------------|\n",
    "|Accuracy|$\\frac{TP +TN}{TP + TN + FP + FN}$|Overall accuracy of the model|\n",
    "|Precision|$\\frac{TP}{TP + FP}$|How many Positives are accurate|\n",
    "|Recall (aka Sensitivity) |$\\frac{TP}{TP + FN}$|Positive sample covered|\n",
    "|Specitivity |$\\frac{TN}{TN + FP}$|Negative sample covered|\n",
    "|F1 Score |$\\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}}$|Precision Recall Score|\n",
    "* __ROC (Receiver Operating Characteristics)__: Plot of TPR, FPR\n",
    "* __AUC (Area Under of the Curve)__: Perfect predictor has TPR = 1.0 when FPR = 0.0. Random predictor has the value of TPR = FPR.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18. How to deal with imbalanced binary response variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Re-sample data__: For logistic regression models unbalanced training data affects only the estimate of the model intercept (although this of course skews all the predicted probabilities, which in turn compromises your predictions). Fortunately the intercept correction is straightforward: Provided you know, or can guess, the true proportion of 0s and 1s and know the proportions in the training set you can apply a rare events correction to the intercept. Details are in King and Zheng (2001).\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "19. What models can be used to predict a binary response variable? What are are the differences between these?\n",
    "21. Why might it be better to include fewer features compared to many?\n",
    "22. Given training data on tweets and their tweets, how would you predict the no. of re-tweets of a given tweet after 7 days after only observing 2 days worth of data?\n",
    "23. How would you construct a feed to show relevant content for a site that involved user interactions with items?\n",
    "24. How would you design the people you may know feature on Linkedin or FB?\n",
    "25. How would you predict who someone may want to send a Snapchat or Gmat to?\n",
    "26. How would you suggest to a franchise where to open a new store?\n",
    "27. In a search engine, query auto complete solution\n",
    "28. Given a database of all previous alumni donations to your university, how would you predict which recent alumni are more likely to donate?\n",
    "29. You're Uber and you want to design a heatmap to recommend to drivers where to wait for passenger. How would you approach this?\n",
    "30. How would you build a model to predict a March Madness bracket?\n",
    "31. You want to run a regression to predict the probability of a flight delay, but there are flights with delays up to 12 hours that are really messing up your model. How will you address this?\n",
    "32. Derive MLE estimation from likelihood function?\n",
    "33. Program a Naive Bayes algorithm\n",
    "34. Program a k-NN algorithm (a) iterative and (b) vectorized\n",
    "35. What are differences between generative and discriminative models? Examples.\n",
    "36. Derive likelihood function for BG NBD\n",
    "37. Typical loss functions used: \n",
    "    a. Least Squared Error\n",
    "    b. Logistic Loss\n",
    "    c. Hinge Loss\n",
    "    d. Cross-entropy\n",
    "38. What is the cost function in gradient descent?\n",
    "39. What is the general form of gradient descent? What are the typical parameters to control gradient descent?\n",
    "40. What is Newton's algorithm? What is the Newton-Rahpson method?\n",
    "41. Derive linear regression co-efficients using Normal equation?\n",
    "42. What is the mathematical form of Least Mean Square Algorithm?\n",
    "43. What is locally weighted regression? \n",
    "44. What is the sigmoid function\n",
    "45. What is the general form of logistic regression\n",
    "46. What is the genral form of softmax regression?\n",
    "47. What are generalized regression models? How can the Bernauli, Gaussian, Poisson, Geomertric distributions be used within GLM?\n",
    "48. What are the assumptions in GLM?\n",
    "49. What is SVM? How does it work?\n",
    "50. How does Gaussian Discriminant analysis work?\n",
    "51. Learning Theory: What is Union Bound?\n",
    "52. Learning Theory: What is Hoeffding inequality?\n",
    "53. Learning Theory: What is Training Error?\n",
    "54. Learning Theory: What is Probably Approximately Correct (PAC)?\n",
    "55. Learning Theory: What is Shattering?\n",
    "56. Learning Theory: What is Upper Bound Theorem?\n",
    "57. Learning Theory: What is VC dimension?\n",
    "58. Learning Theory: What is Vapnik Theorem?\n",
    "59. What is EM algorithm? How is it used in discovering LAten variables?\n",
    "60. What is k-means clustering? How does the algorithm work?\n",
    "61. How to optimize k-means? how to find optimal k?\n",
    "62. What are the assumptions with k-means algorithm?\n",
    "63. What is k-protype algorithm? When is it typically used?\n",
    "64. What is Hierarchical clustering? What are the different linkage functions that can be used?\n",
    "65. What are the metrics that can be used to assess clusters? Solihouette, Calinski-Harabaz?\n",
    "66. What is PCA? How does it work? Code it up\n",
    "67. What is ICA? How does it work?\n",
    "68. What is NN? What is the general form?\n",
    "69. What are the typical activation functions used? Describe Sigmoid, Tanh, ReLU, Leaku ReLU?\n",
    "70. How does the backpropagation algorithm work?\n",
    "    a. Cross-entropy loss\n",
    "    b. Learning Rate\n",
    "    c. Backpropagation\n",
    "    d. Updating weights\n",
    "    e. Dropout\n",
    "71. What is CNN?\n",
    "    a. Convolutional Layer Requirement\n",
    "    b. Batch normalization\n",
    "72. What is RNN?\n",
    "    a. Types of gates? Input, Forget, Gate, Output gate\n",
    "    b. LSTM\n",
    "73. Re-inforcement Learning and Control\n",
    "    a. What is policy\n",
    "    b. What is markov decision process\n",
    "    c. What is value function?\n",
    "    d. What are bellman equation\n",
    "    e. Value iteration algorithm\n",
    "    f. MLE\n",
    "    g. Q-learning\n",
    "74. Classification evaluation metrics:\n",
    "    a. Accuracy, Precision, Recal (Sensitivity), Specificity, F1 Score\n",
    "    b. ROC (TPR aka Recall aka Sensitivity, FPR aka 1- specificity)\n",
    "    c. AUC\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 75. Regression Metrics:\n",
    "* **Total Sum of Squares (TSS):** Measures the variation in the observed data  \n",
    "$ \\sum_{i=0}^{m} (y_i - \\bar{y})^2$   \n",
    "\n",
    "* __Residual Sum of Squares (RSS):__ Measures the variation in the modelling errors  \n",
    "$ \\sum_{i=0}^{m} (y_i - \\hat{y_i})^2$  \n",
    "* __Explained Sum of Squares (ESS):__  Measures variation in the modelled values  \n",
    "$ \\sum_{i=0}^{m} (\\hat{y_i} - \\bar{y})^2$   \n",
    "\n",
    "* $R^2$ or co-efficient of variation   \n",
    "$ (1- \\frac{RSS}{TSS})$  \n",
    "  \n",
    "* Model performance: Mallow's Cp, AIC, BIC, Adjusted R^2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 76. Regularization:\n",
    "* __Definition:__ Regularization is a technique used to reduce the complexity of the model, the objective is to increase bias and avoid overfitting the training sample. Regularization is usually done through addition of regularization term to the cost function of the machine learning model. \n",
    "* __LASSO aka L1 Regularization:__ Lasso shriks co-efficients towards 0. When the $\\lambda$ is sufficiently large, the lasso method is likely to end up  shrinking some of the coefficients to 0. If there is a group of highly correlated variables, Lasso tends to select one from the group and ignore the rest. $$J(\\theta) = \\sum_{i = 1}^{m}{\\left( y_i - \\theta_0 - \\sum_{j=1}^{n} \\theta_j x_{i\\space j}\\right) ^2} + \\lambda \\sum_{j=1}^{n}{\\mid \\theta_j \\mid}$$\n",
    "* __Ridge aka L2 Regulization:__ Ridge regularization shriks co-efficient to 0. But, due to the nature of the penalty term, ridge penalization always yields models that have all the $n$ predictors. $$J(\\theta) = \\sum_{i = 1}^{m}{\\left( y_i - \\theta_0 - \\sum_{j=1}^{n} \\theta_j x_{i\\space j}\\right) ^2} + \\lambda \\sum_{j=1}^{n}{ \\theta_j}^2$$\n",
    "* __Elastic Net:__ L1 regularization is conservative with highly-correlated variables. Elastic net combines the cost function of both L1 and L2 to allow for more flexbility when compared to L1. \n",
    "$$J(\\theta) = \\sum_{i = 1}^{m}{\\left( y_i - \\theta_0 - \\sum_{j=1}^{n} \\theta_j x_{i\\space j}\\right) ^2} + \\lambda_1 \\sum_{j=1}^{n}{\\mid \\theta_j \\mid} + \\lambda_2 \\sum_{j=1}^{n}{ \\theta_j}^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 77. Diagnostics:\n",
    "* Discovering overfitting, underfitting through training and cv error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
