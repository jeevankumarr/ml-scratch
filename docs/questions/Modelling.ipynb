{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is bias variance trade-off?\n",
    "\n",
    "* The Bias-Variance trade-off is the central problem in supervised machine learning. Ideally, one wants to choose a model that accurately captures the regularities in its training data, but also generilizes well to unseen data. Unfortunately, these goals are contradictory, and often impossible to do both. \n",
    "* __Bias__ represents the error as a result of misaligned assumptions in the learning algorithm that do not represent the true relationship between predictors and the response variable. \n",
    "* __Variance__ represents the error from sensitivity to fluctuations in the training set. High variance can cause an algorithm to model the noise in the training data, rather than intended outputs. \n",
    "* __Discussion__ Models with low-bias are usually more complex (ex: higher-order regression polynomials), enabling them to represent the training set from accurately. However, they may also represent the noise in the trianing set, making their predictions less accurate. On the other hand, models with high-bias are simple (ex: linear regression polynomials), but may produce lower variance prediction when applied beyond the training set. \n",
    "* __Sources:__ [wiki](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Derive bias variance trade-off?\n",
    "\n",
    "\n",
    "* __Derivation of Variance:__  \n",
    "$Var(X) = E[(X-E[X])^2]$  \n",
    "$Var(X) = E[(X^2 + E[X]^2 - 2 * X E[X])]$  \n",
    "$Var(X) = E[X^2] + E[X]^2 - 2 * E[X] * E[X]$  \n",
    "$Var(X) = E[X^2] - E[X]^2$  \n",
    "\n",
    "* __Variance in $y$__  \n",
    "$ Var(y) = E[ (y - E[y])^2]$  \n",
    "$ Var(y) = E[ (E[y] + e - E[y])^2]$  \n",
    "$ Var(y) = E[e^2]$  \n",
    "$ Var(y) = Var(e) + E[e]^2$; But: $E[e] = 0$  \n",
    "$ Var(y) = Var(e)$  \n",
    "\n",
    "* __Sum of Squares Error:__  \n",
    "$SS = E[(y - \\hat{y})^2]$  \n",
    "$SS = E[(y^2 + \\hat{y}^2 - 2 * y * \\hat{y}]$  \n",
    "$SS = E[y^2] + E[\\hat{y}^2] - 2 E(y*\\hat{y})$  \n",
    "$SS = Var(y) + E[y]^2  + Var(\\hat{y}) + E[\\hat{y}]^2  - 2 E(y*\\hat{y})$\n",
    "$SS = Var(e) + Var(\\hat{y}) + \\left( E[y] - E[\\hat{y}]\\right)^2$  \n",
    "$SS = irreducible\\space error + Variance + Bias^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Linear regression derivation\n",
    "4. Linear regression derivation based on matrix\n",
    "5. Write CART algorithm\n",
    "6. Write Bagged Tree algorithm\n",
    "7. Write Random Forest algorithm\n",
    "8. Write Gradient boosting algorithm\n",
    "9. Derive NN - backpropagation\n",
    "10. What is SVN?\n",
    "11. What is ARIMA formula structure? What are the steps to take in trianing a good time series model? What are alternatives to ARIMA?\n",
    "12. How would you tell whether a new feature X is beneficial or not  \n",
    "13. Create a model to predict a response variable\n",
    "14. What could be issues if distribution of test data is significantly different from training data?\n",
    "15. How to make model more robust to outliers?\n",
    "16. MSE vs. MAD why choose one? Differences in model that uses one over the other. \n",
    "17. How to evaluate a binary classifier?\n",
    "18. How to deal with imbalanced binary response variable?\n",
    "19. What models can be used to predict a binary response variable? What are are the differences between these?\n",
    "20. What is regularization and where might it be helpful? What is an example of regularization in a model?\n",
    "21. Why might it be better to include fewer features compared to many?\n",
    "22. Given training data on tweets and their tweets, how would you predict the no. of re-tweets of a given tweet after 7 days after only observing 2 days worth of data?\n",
    "23. How would you construct a feed to show relevant content for a site that involved user interactions with items?\n",
    "24. How would you deisgn the people you may know feature on Linkedin or FB?\n",
    "25. How would you predict who someone may want to send a Snapchat or Gmat to?\n",
    "26. How would you suggest to a franchise where to open a new store?\n",
    "27. In a search engine, query auto complete solution\n",
    "28. Given a database of all previous alumni donations to your university, how would you predict which recent alumni are more likely to donate?\n",
    "29. You're Uber and you want to design a heatmap to recommend to drivers where to wait for passenger. How would you approach this?\n",
    "30. How would you build a model to predict a March Madness bracket?\n",
    "31. You want to run a regression to predict the probability of a flight delay, but there are flights with delays up to 12 hours that are really messing up your model. How will you address this?\n",
    "32. Derive MLE estimation from likelihood function?\n",
    "33. Program a Naive Bayes algorithm\n",
    "34. Program a k-NN algorithm (a) iterative and (b) vectorized\n",
    "35. What are differences between generative and discriminative models? Examples.\n",
    "36. Derive likelihood function for BG NBD\n",
    "37. Typical loss functions used: \n",
    "    a. Least Squared Error\n",
    "    b. Logistic Loss\n",
    "    c. Hinge Loss\n",
    "    d. Cross-entropy\n",
    "38. What is the cost function in gradient descent?\n",
    "39. What is the general form of gradient descent? What are the typical parameters to control gradient descent?\n",
    "40. What is Newton's algorithm? What is the Newton-Rahpson method?\n",
    "41. Derive linear regression co-efficients using Normal equation?\n",
    "42. What is the mathematical form of Least Mean Square Algorithm?\n",
    "43. What is locally weighted regression? \n",
    "44. What is the sigmoid function\n",
    "45. What is the general form of logistic regression\n",
    "46. What is the genral form of softmax regression?\n",
    "47. What are generalized regression models? How can the Bernauli, Gaussian, Poisson, Geomertric distributions be used within GLM?\n",
    "48. What are the assumptions in GLM?\n",
    "49. What is SVM? How does it work?\n",
    "50. How does Gaussian Discriminant analysis work?\n",
    "51. Learning Theory: What is Union Bound?\n",
    "52. Learning Theory: What is Hoeffding inequality?\n",
    "53. Learning Theory: What is Training Error?\n",
    "54. Learning Theory: What is Probably Approximately Correct (PAC)?\n",
    "55. Learning Theory: What is Shattering?\n",
    "56. Learning Theory: What is Upper Bound Theorem?\n",
    "57. Learning Theory: What is VC dimension?\n",
    "58. Learning Theory: What is Vapnik Theorem?\n",
    "59. What is EM algorithm? How is it used in discovering LAten variables?\n",
    "60. What is k-means clustering? How does the algorithm work?\n",
    "61. How to optimize k-means? how to find optimal k?\n",
    "62. What are the assumptions with k-means algorithm?\n",
    "63. What is k-protype algorithm? When is it typically used?\n",
    "64. What is Hierarchical clustering? What are the different linkage functions that can be used?\n",
    "65. What are the metrics that can be used to assess clusters? Solihouette, Calinski-Harabaz?\n",
    "66. What is PCA? How does it work? Code it up\n",
    "67. What is ICA? How does it work?\n",
    "68. What is NN? What is the general form?\n",
    "69. What are the typical activation functions used? Describe Sigmoid, Tanh, ReLU, Leaku ReLU?\n",
    "70. How does the backpropagation algorithm work?\n",
    "    a. Cross-entropy loss\n",
    "    b. Learning Rate\n",
    "    c. Backpropagation\n",
    "    d. Updating weights\n",
    "    e. Dropout\n",
    "71. What is CNN?\n",
    "    a. Convolutional Layer Requirement\n",
    "    b. Batch normalization\n",
    "72. What is RNN?\n",
    "    a. Types of gates? Input, Forget, Gate, Output gate\n",
    "    b. LSTM\n",
    "73. Re-inforcement Learning and Control\n",
    "    a. What is policy\n",
    "    b. What is markov decision process\n",
    "    c. What is value function?\n",
    "    d. What are bellman equation\n",
    "    e. Value iteration algorithm\n",
    "    f. MLE\n",
    "    g. Q-learning\n",
    "74. Classification evaluation metrics:\n",
    "    a. Accuracy, Precision, Recal (Sensitivity), Specificity, F1 Score\n",
    "    b. ROC (TPR aka Recall aka Sensitivity, FPR aka 1- specificity)\n",
    "    c. AUC\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 75. Regression Metrics:\n",
    "* **Total Sum of Squares (TSS):** Measures the variation in the observed data $$ \\sum_{i=0}^{m} (y_i - \\bar{y})^2$$  \n",
    "* __Residual Sum of Squares (RSS):__ Measures the variation in the modelling errors $$ \\sum_{i=0}^{m} (y_i - \\hat{y_i})^2$$  \n",
    "* __Explained Sum of Squares (ESS):__  Measures variation in the modelled values $$ \\sum_{i=0}^{m} (\\hat{y_i} - \\bar{y})^2$$ \n",
    "* $R^2$ = $$ (1- \\frac{RSS}{TSS})$$  \n",
    "* Model performance: Mallow's Cp, AIC, BIC, Adjusted R^2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 76. Regularization:\n",
    "* __Definition:__ Regularization is a technique used to reduce the complexity of the model, the objective is to increase bias and avoid overfitting the training sample. Regularization is usually done through addition of regularization term to the cost function of the machine learning model. \n",
    "* __LASSO aka L1 Regularization:__ Lasso shriks co-efficients towards 0. When the $\\lambda$ is sufficiently large, the lasso method is likely to end up  shrinking some of the coefficients to 0. If there is a group of highly correlated variables, Lasso tends to select one from the group and ignore the rest. $$J(\\theta) = \\sum_{i = 1}^{m}{\\left( y_i - \\theta_0 - \\sum_{j=1}^{n} \\theta_j x_{i\\space j}\\right) ^2} + \\lambda \\sum_{j=1}^{n}{\\mid \\theta_j \\mid}$$\n",
    "* __Ridge aka L2 Regulization:__ Ridge regularization shriks co-efficient to 0. But, due to the nature of the penalty term, ridge penalization always yields models that have all the $n$ predictors. $$J(\\theta) = \\sum_{i = 1}^{m}{\\left( y_i - \\theta_0 - \\sum_{j=1}^{n} \\theta_j x_{i\\space j}\\right) ^2} + \\lambda \\sum_{j=1}^{n}{ \\theta_j}^2$$\n",
    "* __Elastic Net:__ L1 regularization is conservative with highly-correlated variables. Elastic net combines the cost function of both L1 and L2 to allow for more flexbility when compared to L1. \n",
    "$$J(\\theta) = \\sum_{i = 1}^{m}{\\left( y_i - \\theta_0 - \\sum_{j=1}^{n} \\theta_j x_{i\\space j}\\right) ^2} + \\lambda_1 \\sum_{j=1}^{n}{\\mid \\theta_j \\mid} + \\lambda_2 \\sum_{j=1}^{n}{ \\theta_j}^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 77. Diagnostics:\n",
    "* Discovering overfitting, underfitting through training and cv error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
