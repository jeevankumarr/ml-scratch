{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Based Questions\n",
    "\n",
    "### Likely To Lapse Models\n",
    "\n",
    "#### Objective\n",
    "* Improve retention through intervening with players who are likely to lapse\n",
    "* Identifying players who are likely to lapse and \n",
    "* Identify key features that are predictive of likelihood to lapse\n",
    "\n",
    "#### What was the definition of lapse?\n",
    "* For the purposes of the model, the targe feature was modelled as a user who will login in the next 7 days and does not login in the 7 days after next week\n",
    "* A key business metric that is used in Zynga is the CURR (current user retention rate), which is defined as a player who logged-in in the past 7 days and during the 7 days prior to last week. \n",
    "* A player is considered lapsed if he / she has logged-in in the past 7 days and does not login in any of the next 7 days\n",
    "\n",
    "#### What were the features used?\n",
    "* Time Windowing: -1, -2, -3, -4 week\n",
    "* LB Spins, MLB Spins, \n",
    "* Cash Hands, Tourn Hands, \n",
    "* Challenges completed, tickets redeemed\n",
    "* Chat used, chat count\n",
    "* Player level, changes\n",
    "* Login days, hands played days, purch days\n",
    "* Chips purchased, gold purchased\n",
    "* txn ct, chips txn ct, gold txn ct\n",
    "* Engg: momentum features (-3/-4, -2/-3, -1/-2)\n",
    "\n",
    "#### What was the process adopted to treat missing values?\n",
    "* Tourn Hands were Null for most players. Replacement with 0. \n",
    "* Although the chat used flag was 1, chat count was sometimes 0, this was corrected\n",
    "* \n",
    "\n",
    "#### What was the process used to deal with correlated features?\n",
    "* There were lots of features that were correlated for ex: chated and chat count. Typical decision was to use the more informative variable for ex: chat count\n",
    "* But, some values such gold txn count, and total txn count, in this case, ingredient features gold ct and chips txn ct were used inplace of the added up ct. During iterations, either all the ingredient features were used or only the aggregated feature was used. \n",
    "* Part of the goal was to derive importance and check the partial dependency for each of the features, hence we used the ingredient features a lot more than the aggregated features\n",
    "\n",
    "#### What was the process used for any variable transformations?\n",
    "* count variables like hands played, login days are all right skewed, and have a long-right tail\n",
    "\n",
    "#### What were the techniques considered for prediction?\n",
    "* Logistic Regression, Random Forest, XGBoost, GBT\n",
    "\n",
    "#### What are the weakness of each technique?\n",
    "* Logistic Regression: \n",
    "    * __Pros:__ Intrepretability\n",
    "    * __Con:__ High Bias, Less accurate, no multi collenearity, missing value treatment needs to be meaningful \n",
    "* Random Forest:\n",
    "    * __Pros:__ Low Bias, robust to correlated features, can handle thousands of features, sensitive to training sample\n",
    "    * __Con:__ Can overfit, interpretability is low, slow\n",
    "    \n",
    "\n",
    "#### Which was the technique used and why?\n",
    "* Random Forest (payer) and XGBoost (Non-payer)\n",
    "    * It was chosen based on performance and tuning\n",
    "    * We tried to maximize Recall because the purpose of the project was to intervene for all players who are likely to lapse. We kept precision above 0.7 to avoid too many false positives. \n",
    "    \n",
    "#### How were the hyper parameters tuned?\n",
    "* Random Forest\n",
    "    * __criterion__: gini, cross entropy\n",
    "        * gini $\\sum_{i=1}^{n_{class}} p_i (1-p_i)$\n",
    "        * cross entropy $\\sum_{i=1}^{n_{class}}-p_i \\log{p_i}$\n",
    "    * __n_estimators__: No. of trees. More trees reduces variance. Averaging over many trees reduces variance. \n",
    "    * __min samples leaf count__: no. of samples in the leaf. Increasing it reduces the height of the trees. Which also reduces overfitting. \n",
    "    * __min samples split__: \n",
    "    * max depth: max depth of the random forest. If a tree is too deep it overfits. We saw about 3-5 depth had the impact we needed. \n",
    "    * __max features__: typically sqrt(feature_ct), or log2(feature_ct)\n",
    "\n",
    "    * __max leaf nodes__: Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
    "    * __min impurity decrease__:A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "    * __bootstrap or sub sample__: Whether bootstrap samples are used when building trees.\n",
    "    * __oob_score__: This method simply tags every observation used in different tress. And then it finds out a maximum vote score for every observation based on only trees which did not use this particular observation to train itself.\n",
    "    * __class_weight__: 'balanced' or 'balanced_subsample' or None\n",
    "\n",
    "#### What problems does each of the hypermeter solve?\n",
    "\n",
    "#### How does RandomForest work?\n",
    "* \n",
    "#### How does Gradient Boosted Trees work?\n",
    "\n",
    "#### How does XGBoost work?\n",
    "\n",
    "#### What insights were derived from the model?\n",
    "\n",
    "#### How would you assess the output of logistic regression?\n",
    "\n",
    "#### Derive the estimation procedure for logistic regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### How does maximum likelihood work?\n",
    "In this discussion, we will lay down the foundational principles that enable the optimal estimation of a given algorithm’s parameters using maximum likelihood estimation and gradient descent. Using the logistic regression, we will first walk through the mathematical solution, and subsequently we shall implement our solution in code.\n",
    "\n",
    "Maximum Likelihood\n",
    "The logistic model uses the sigmoid function (denoted by sigma) to estimate the probability that a given sample y belongs to class 1 given inputs X and weights W,\n",
    "\n",
    " $$P(y=1∣x)=\\sigma(W^TX)$$\n",
    "where the sigmoid of our activation function for a given n is:\n",
    "\n",
    "$$y_i=\\sigma(a_i)=\\frac{1}{1+e^{−a_i}}$$\n",
    "The accuracy of our model predictions can be captured by the objective function L, which we are trying to maxmize.\n",
    "\n",
    "$$L=\\prod_{i=1}^M y_i^{t_i}(1−y_i)^{1−t_i}$$\n",
    "If we take the log of the above function, we obtain the maximum log likelihood function, whose form will enable easier calculations of partial derivatives. Specifically, taking the log and maximizing it is acceptable because the log likelihood is monotomically increasing, and therefore it will yield the same answer as our objective function.\n",
    "\n",
    "$$L=\\sum_{i=1}^M t_i \\log{y_i} + (1−t_i)\\log(1−y_i)$$\n",
    "In our example, we will actually convert the objective function (which we would try to maximize) into a cost function (which we are trying to minimize) by converting it into the negative log likelihood function:\n",
    "$$J=-\\sum_{i=1}^M t_i \\log{y_i} + (1-t_i)\\log{1-y_i}$$\n",
    "\n",
    "__Gradient Descent__\n",
    "Once we have an objective function, we can generally take its derivative with respect to the parameters (weights), set it equal to zero, and solve for the parameters to obtain the ideal solution. However, in the case of logistic regression (and many other complex or otherwise non-linear systems), this analytical method doesn’t work. Instead, we resort to a method known as gradient descent, whereby we randomly initialize and then incrementally update our weights by calculating the slope of our objective function. When applying the cost function, we want to continue updating our weights until the slope of the gradient gets as close to zero as possible. We can show this mathematically:\n",
    "$$w = w + \\delta w$$\n",
    "\n",
    "where the second term on the right is defined as the learning rate times the derivative of the cost function with respect to the the weights (which is our gradient):\n",
    "\n",
    " $$\\delta w=η \\delta J(w)$$\n",
    "Thus, we want to take the derivative of the cost function with respect to the weight, which, using the chain rule, gives us:\n",
    "\n",
    "$$\\frac{\\partial J }{\\partial w_j} = \\sum_{i=1}^{N} \\frac{\\partial J}{\\partial y_i} \\frac{\\partial y_i}{\\partial a_i} \\frac{\\partial a_i}{\\partial w_j}\\$$\n",
    "\n",
    "Thus, we are looking to obtain three different derivatives. Let us start by solving for the derivative of the cost function with respect to y:\n",
    "$$\\frac{\\partial J}{\\partial y_i} = t_i \\frac{1}{y_i} + (1-t_i) \\frac{-1}{1-y_i} = \\frac{t_i}{y_i}-\\frac{1-t_i}{1-y_i}$$\n",
    "\n",
    "Next, let us solve for the derivative of y with respect to our activation function:\n",
    "\n",
    "$$y_i=\\sigma(a_i)=\\frac{1}{1+e^{−a_i}}$$\n",
    "$$\\frac{\\partial y_i}{\\partial a_i} = y_i (1-y_i)$$\n",
    "\n",
    "And lastly, we solve for the derivative of the activation function with respect to the weights:\n",
    "$$a_i = W^T X_n$$\n",
    "$$\\frac{a_i}{w_j} = x_{ij}$$\n",
    "\n",
    "Now we can put it all together and simply.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_j} = - \\sum_{i=1}^M \\frac{t_i}{y_i} y_i (1-y_i) x_{ij} - \\frac{1-t_i}{1-y_i} y_i (1-y_i) x_{ij}$$\n",
    "We can get rid of the summation above by applying the principle that a dot product between two vectors is a summover sum index. That is:\n",
    "$$a^T b = \\sum_{i=1}^M a_i b_i$$\n",
    "\n",
    "Therefore, the gradient with respect to w is:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w} = X^T (Y-T)$$\n",
    "\n",
    "\n",
    "\n",
    "If you are asking yourself where the bias term of our equation (w0) went, we calculate it the same way, except our x becomes 1. We will demonstrate how this is dealt with practically in the subsequent section.\n",
    "\n",
    "∂J∂w0=∑n=1N(yn−tn)xn0=∑n=1N(yn−tn)\n",
    "Coded Example\n",
    "We shall now use a practical example to demonstrate the application of our mathematical findings. We will create a basic linear regression model with 100 samples and two inputs. Our inputs will be random normal variables, and we will center the first 50 inputs around (-2, -2) and the second 50 inputs around (2, 2). These two clusters will represent our targets (0 for the first 50 and 1 for the second 50), and because of their different centers, it means that they will be linearly separable.\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 100\n",
    "D = 2\n",
    "\n",
    "X = np.random.randn(N,D)\n",
    "\n",
    "# center the first 50 points at (-2,-2)\n",
    "X[:50,:] = X[:50,:] - 2*np.ones((50,D))\n",
    "\n",
    "# center the last 50 points at (2, 2)\n",
    "X[50:,:] = X[50:,:] + 2*np.ones((50,D))\n",
    "\n",
    "# labels: first 50 are 0, last 50 are 1\n",
    "T = np.array([0]*50 + [1]*50)\n",
    "\n",
    "# In order to easily deal with the bias term, we will simply add another N-by-1 vector of ones to our input matrix.\n",
    "# add a column of ones\n",
    "# ones = np.array([[1]*N]).T # old\n",
    "ones = np.ones((N, 1))\n",
    "Xb = np.concatenate((ones, X), axis=1)\n",
    "\n",
    "# randomly initialize the weights\n",
    "w = np.random.randn(D + 1)\n",
    "\n",
    "# calculate the model output\n",
    "z = Xb.dot(w)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "Y = sigmoid(z)\n",
    "\n",
    "# calculate the cross-entropy error\n",
    "def cross_entropy(T, Y):\n",
    "    E = 0\n",
    "    for i in xrange(N):\n",
    "        if T[i] == 1:\n",
    "            E -= np.log(Y[i])\n",
    "        else:\n",
    "            E -= np.log(1 - Y[i])\n",
    "    return E\n",
    "\n",
    "# let's do gradient descent 100 times\n",
    "learning_rate = 0.1\n",
    "for i in xrange(100):\n",
    "    if i % 10 == 0:\n",
    "        print cross_entropy(T, Y)\n",
    "\n",
    "    # gradient descent weight udpate\n",
    "    w += learning_rate * Xb.T.dot(T - Y)\n",
    "\n",
    "    # recalculate Y\n",
    "    Y = sigmoid(Xb.dot(w))\n",
    "\n",
    "print(\"Final w:\", w)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### What is the difference between MLE and gradient descent\n",
    "__Maximum likelihood estimation__ is a general approach to estimating parameters in statistical models by maximizing the likelihood function defined as\n",
    "\n",
    "$$L(\\theta|𝑋)=f(𝑋|\\theta)$$\n",
    "that is, the probability of obtaining data 𝑋 given some value of parameter 𝜃. Knowing the likelihood function for a given problem you can look for such 𝜃 that maximizes the probability of obtaining the data you have. Sometimes we have known estimators, e.g. arithmetic mean is an MLE estimator for 𝜇 parameter for normal distribution, but in other cases you can use different methods that include using optimization algorithms. ML approach does not tell you how to find the optimal value of 𝜃 -- you can simply take guesses and use the likelihood to compare which guess was better -- it just tells you how you can compare if one value of 𝜃 is \"more likely\" than the other. This is treated as a cost function in gradient descent optimization algorithm. \n",
    "\n",
    "__Gradient descent__ is an optimization algorithm. You can use this algorithm to find minimum (or maximum, then it is called gradient ascent) of many different functions. The algorithm does not really care what is the function that it minimizes, it just does what it was asked for. So with using optimization algorithm you have to know somehow how could you tell if one value of the parameter of interest is \"better\" than the other. You have to provide your algorithm some function to minimize and the algorithm will deal with finding its minimum.\n",
    "\n",
    "You can obtain maximum likelihood estimates using different methods and using an optimization algorithm is one of them. On another hand, gradient descent can be also used to maximize functions other than likelihood function.  \n",
    "\n",
    "Ref: [link](https://stats.stackexchange.com/questions/183871/what-is-the-difference-between-maximum-likelihood-estimation-gradient-descent)\n",
    "#### What is the differences between GD, BGD, MBGD, SGD?\n",
    "\n",
    "#### What are alternative models to estimating lapse behavior?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Txn Prediction and LTV\n",
    "\n",
    "#### What was the objective of the project?\n",
    "* Poker wanted to target users with very low LTV or expected low txn with interstitials \n",
    "* Poker wanted to estimate how much residual revenue was present in the current payer base\n",
    "* Poker wanted to detect which users are likely to have high LTV\n",
    "\n",
    "#### What were the outputs generated?\n",
    "\n",
    "#### What was the technique adopted and why?\n",
    "\n",
    "#### How does BG / NBD work?\n",
    "\n",
    "#### How does Gamma / Gamma model work?\n",
    "\n",
    "#### What were the alternatives considered?\n",
    "\n",
    "#### What was the validation and how was it performed?\n",
    "\n",
    "#### What is the likelihood function for BG|NDB?\n",
    "\n",
    "#### What is the likelihood function for Gamma Gamma?\n",
    "\n",
    "#### What were the insights gathered from the model?\n",
    "\n",
    "\n",
    "#### What were the experiments conducted to achieve the stated objectives?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Feature Engg\n",
    "\n",
    "#### What was the objective of the project?\n",
    "\n",
    "#### What was done?\n",
    "\n",
    "#### What were the ouputs created?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bot or not model\n",
    "\n",
    "#### What was the objective of the model?\n",
    "\n",
    "#### What was the bot signatures calculated?\n",
    "\n",
    "#### How was the source of truth established?\n",
    "\n",
    "#### How much of the fraud was detected?\n",
    "\n",
    "#### What was the impact of the detection?\n",
    "\n",
    "#### How was the model scaled to millions of users?\n",
    "\n",
    "#### What was the techniques used to do the prediction?\n",
    "\n",
    "#### How were the hyperparameters tuned?\n",
    "\n",
    "#### What was the scoring frequency?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bid Recommendation Service\n",
    "\n",
    "#### What was the objective of the service?\n",
    "\n",
    "#### What was the optimization technique used?\n",
    "\n",
    "\n",
    "#### How was local optimization avoided?\n",
    "\n",
    "#### Which partners were optimized?\n",
    "\n",
    "\n",
    "#### How was validation done?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting System for KPIs\n",
    "#### What were the objectives?\n",
    "#### What was the techniques used?\n",
    "#### What were the assumptions made about the data?\n",
    "#### How were the assumptions tests?\n",
    "#### How was the validation done?\n",
    "#### How were the results tracked and monitored?\n",
    "#### What were the issues?\n",
    "\n",
    "#### What were the alternate models considered?\n",
    "#### How could the system be improved?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian AB Testing library\n",
    "#### What was the purpose of the library\n",
    "#### What were the underlying assumptions?\n",
    "#### What is the theory behind Bayesian AB testing\n",
    "#### How were different types of variables treated for AB testing proportion, count, continuous?\n",
    "\n",
    "#### What is the meanning of conjugate prior?\n",
    "#### Why is Beta a prior to Binomial distribution?\n",
    "\n",
    "#### What is Thompson sampling?\n",
    "#### How was this model used to make decisions?\n",
    "\n",
    "#### What were the alternatives considered?\n",
    "\n",
    "#### How does this approach compare to frequentist?\n",
    "\n",
    "#### How is Type I error handled in Bayesian statistics?\n",
    "\n",
    "#### How is Type II error handled in Bayesian statistics?\n",
    "\n",
    "#### How does this compare with using Chi-Square test or Anova?\n",
    "\n",
    "#### What is the shortcomings of the Bayesian approach?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight Delay Prediction\n",
    "\n",
    "### Media Mix Modelling\n",
    "\n",
    "### Forecasting sales\n",
    "\n",
    "### Marketing Budget Allocation\n",
    "\n",
    "### Product Upsell Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
